# Promising false friends pairs:
# Note: From only starting the search for false friends language pairs, words that also sound the same but have a different meaning are also catagorized as false friends.
# English and Spanish have lots of false friends words that fall under that catagory, but, the words themselves seem to be spelled quite differently
# English-German https://en.wiktionary.org/wiki/Appendix:False_friends_between_English_and_German
# English-French https://en.wiktionary.org/wiki/Appendix:False_friends_between_English_and_French
import shutil
import sys
import os
import csv
import matplotlib.pyplot as plt
import random
from SaGe_main.src.sage_tokenizer import *
from tokenizers import Tokenizer
from tokenizers.models import BPE, Unigram, WordPiece, WordLevel
from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordPieceTrainer, WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace

def create_multi_text_file(path1, path2, file_name, num_rows, seed=42):
    """
    Creates a .txt file that combines two different text files by randomly sampling half of the lines
    from each input file using a specific random seed.

    :param path1: Path to file of first language
    :param path2: Path to file of second language
    :param file_name: Name of the combined output file
    :param num_rows: Total number of rows in the output file (half from each input)
    :param seed: Random seed for reproducibility
    """
    rows_from_each = num_rows // 2
    
    with open(path1, 'r', encoding='utf-8') as f1:
        lines1 = f1.readlines()
    with open(path2, 'r', encoding='utf-8') as f2:
        lines2 = f2.readlines()
    
    random.seed(seed)
    sampled1 = random.sample(lines1, rows_from_each)
    random.seed(seed + 1)
    sampled2 = random.sample(lines2, rows_from_each)
    
    with open(file_name, 'w', encoding='utf-8') as f_out:
        f_out.writelines(sampled1 + sampled2)

    
def get_SP_tokenizer(algo, vocab_size, corpus_file_path):
    """
    Get a trained tokenizer on a corpus
    :param algo: the type of tokenizer
    :param vocab_size: the size of the vocabulary
    :param corpus_file_path: a list that contains file paths of corpora
    :return: a trained tokenizer
    """
    
    if "BPE" in algo:
        tokenizer = Tokenizer(BPE(unk_token=unk_token))
        trainer = BpeTrainer(special_tokens=spl_tokens, vocab_size=vocab_size)
    elif "UNI" in algo:
        tokenizer = Tokenizer(Unigram())
        trainer = UnigramTrainer(unk_token=unk_token, special_tokens=spl_tokens, vocab_size=vocab_size)
    elif "WPC" in algo:
        tokenizer = Tokenizer(WordPiece(unk_token=unk_token))
        trainer = WordPieceTrainer(special_tokens=spl_tokens, vocab_size=vocab_size)
    else: #WLVL
        tokenizer = Tokenizer(WordLevel(unk_token=unk_token))
        trainer = WordLevelTrainer(special_tokens=spl_tokens, vocab_size=vocab_size)
    
    tokenizer.pre_tokenizer = Whitespace()
    tokenizer.train(corpus_file_path, trainer)
    return tokenizer
    
def get_sage_tokenizer(algo, schedule, initial_vocab_size, final_vocab_size, corpus_file_path, vocab_file_path, experiment_name):
    """
    
    :param algo:
    :param schedule:
    :param initial_vocab_size:
    :param final_vocab_size:
    :param corpus_file_path:
    :param vocab_file_path:
    :param experiment_name:
    :return: "BPE_SAGE
    """
    
    vocab_builder_tokenizer = algo.split("_")[0]
    tokenizer = get_SP_tokenizer(vocab_builder_tokenizer, initial_vocab_size, corpus_file_path)
    vocab = sorted(list(tokenizer.get_vocab().keys()))
    hexed_vocab = add_single_bytes(hex_vocab(vocab))
    max_len = max([len(bytes.fromhex(str(v))) for v in hexed_vocab])
    
    with open(vocab_file_path, 'w', encoding='utf-8') as vocab_file:
        for hexed_v in hexed_vocab:
            vocab_file.write(f"{hexed_v}\n")
    
    trainer = SaGeVocabBuilder(full_vocab_schedule=schedule,
                               embeddings_schedule=schedule,
                               workers_number=4, max_len=max_len)
    
    trainer.build_vocab(experiment_name=experiment_name, corpus_filepath=corpus_file_path[0],
                        vocabulary_filepath=vocab_file_path)
    with open(f"./results/{experiment_name}/sage_vocabs/active_vocab_{final_vocab_size}.vocab", "r") as f:
        initial_vocab = [bytes.fromhex(line.strip()) for line in f]
    tokenizer = SaGeTokenizer(initial_vocabulary=initial_vocab)
    
    return tokenizer
        
        
def hex_vocab(vocab):
    """
    Translates the SaGE vocabulary to hexadecimal format
    :param vocab: list of vocabulary words generated by BPE or UNI or other tokenizers
    :return: list of hexadecimal vocabulary
    """
    hexed_vocab = []
    for v in vocab:
        hex_token = v.encode("utf-8").hex()
        hexed_vocab.append(hex_token)
    return hexed_vocab

def add_single_bytes(vocab):
    """
    SaGe requires all single bytes to be in the vocabulary. This function adds them in to the vocabulary in hexadecimal
    format, if needed
    :param vocab: list of hexadecimal vocabulary
    :return: updated vocabulary
    """
    for i in range(256):
        t = f"{i:02x}"
        if t not in vocab:
            vocab.append(t)
    return vocab

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)
        print(f"created directory {path}")

def create_experiments_dir(wd, l1, algorithms, experiments, vocab_size):
    """
    This function creates directories and subdirectories for experiments
    :param wd: the working directory
    :param l1: l1 language
    :param algorithms: list of algorithms
    :param experiments: list of experiments [(l2, train_data_path, ff_data_path), ...]
    :param vocab_size: the tokenizer vocab size
    :return:
    """
    base_results = os.path.join(wd, "results")
    base_analysis = os.path.join(wd, "analysis")
    base_experiments = os.path.join(wd, "experiments")

    # Create main directories
    create_dir(base_analysis)
    create_dir(os.path.join(base_analysis, str(vocab_size)))

    create_dir(base_experiments)
    create_dir(os.path.join(base_experiments, str(vocab_size)))
    create_dir(os.path.join(base_experiments, str(vocab_size), l1))

    # Create results dirs for l1 and algorithms with "SAGE"
    for algo in algorithms:
        if "SAGE" in algo:
            create_dir(os.path.join(base_results, f"{l1}_{algo}_{vocab_size}"))

    for experiment in experiments:
        l2 = experiment[0]
        analysis_l2_dir = os.path.join(base_analysis, str(vocab_size), f"{l1}_{l2}")
        create_dir(analysis_l2_dir)
        create_dir(os.path.join(analysis_l2_dir, "graphs"))
        create_dir(os.path.join(analysis_l2_dir, "tokenization"))

        experiments_l2_dir = os.path.join(base_experiments, str(vocab_size), f"{l1}_{l2}")
        create_dir(experiments_l2_dir)

        for algo in algorithms:
            if "SAGE" in algo:
                create_dir(os.path.join(base_results, f"{l2}_{algo}_{vocab_size}"))
                create_dir(os.path.join(base_results, f"{l1}_{l2}_{algo}_{vocab_size}"))

def get_experiments(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        experiments = f.readlines()
    for i in range(len(experiments)):
        l, train_data_path, ff_data_path = experiments[i].split(",")
        train_data_path = train_data_path.strip().replace("\\", "/")
        ff_data_path = ff_data_path.strip().replace("\\", "/")
        experiments[i] = (l, train_data_path, ff_data_path)
    return experiments

def save_tokenizer(tokenizer, dir):
    tokenizer.save(f"{dir}")

def train_l1_tokenizers(l1, algorithms, schedule, initial_vocab_size, final_vocab_size, corpus_file_path):
    
    en_tokenizers = dict()
    for algo in algorithms:
        if "SAGE" in algo:
            tokenizer = get_sage_tokenizer(algo, schedule, initial_vocab_size, final_vocab_size, corpus_file_path,
                                           f"./results/{l1}_{algo}_{vocab_size}/initial_vocab.vocab", f"{l1}_{algo}_{vocab_size}")
            shutil.copy(f"./results/{l1}_{algo}_{vocab_size}/sage_vocabs/active_vocab_{vocab_size}.vocab",
                        f"./experiments/{vocab_size}/{l1}/{l1}_{algo}.vocab")
        else:
            tokenizer = get_SP_tokenizer(algo, final_vocab_size, corpus_file_path)
            save_tokenizer(tokenizer, f"./experiments/{vocab_size}/{l1}/{l1}_{algo}.json")
        en_tokenizers[algo] = tokenizer
    return en_tokenizers
    
if __name__ == '__main__':
    
    num_rows = 300000
    vocab_size = 3000
    initial_sage_vocab_size = 8000
    schedule = [3000, 4000]
    unk_token = "<UNK>"  # token for unknown words
    spl_tokens = ["<UNK>", "<SEP>", "<MASK>", "<CLS>"]  # special tokens
    cwd = os.getcwd().replace("\\", "/")
    l1 = "en"
    experiments = get_experiments("./args_script.txt")
    algorithms, l1_file_path = sys.argv[1:]
    l1_file_path = l1_file_path.strip().replace("\\", "/")
    algorithms = algorithms.split(",")
    create_experiments_dir(cwd, l1, algorithms, experiments, vocab_size)
    # Training l1 tokenizers
    l1_tokenizers = train_l1_tokenizers(l1, algorithms, schedule, initial_sage_vocab_size, vocab_size, [l1_file_path])

    for experiment in experiments:
        l2, l2_file_path, ff_file_path = experiment
        multi_text_file = f"{cwd}/training_data/{l1}_{l2}/{l1}_{l2}_corpus.txt"
        print(f"created multi-text file {l1}_{l2}")
        # Creating multilingual training data
        create_multi_text_file(l1_file_path, l2_file_path, multi_text_file, num_rows)
        for algo in algorithms:
            print(f"starting experiment {l1}_{l2}_{algo}")
            if "SAGE" in algo:
                l2_tokenizer = get_sage_tokenizer(algo, schedule, initial_sage_vocab_size, vocab_size, [l2_file_path],
                                                  f"./results/{l2}_{algo}_{vocab_size}/initial_vocab.vocab", f"{l2}_{algo}_{vocab_size}")
                l1_l2_tokenizer = get_sage_tokenizer(algo, schedule, initial_sage_vocab_size, vocab_size, [multi_text_file],
                                                     f"./results/{l1}_{l2}_{algo}_{vocab_size}/initial_vocab.vocab", f"{l1}_{l2}_{algo}_{vocab_size}")

                shutil.copy(f"./results/{l2}_{algo}_{vocab_size}/sage_vocabs/active_vocab_{vocab_size}.vocab",
                            f"./experiments/{vocab_size}/{l1}_{l2}/{l2}_{algo}.vocab")
                shutil.copy(f"./results/{l1}_{l2}_{algo}_{vocab_size}/sage_vocabs/active_vocab_{vocab_size}.vocab",
                            f"./experiments/{vocab_size}/{l1}_{l2}/{l1}_{l2}_{algo}.vocab")
            else:
                l2_tokenizer = get_SP_tokenizer(algo, vocab_size, [l2_file_path])
                l1_l2_tokenizer = get_SP_tokenizer(algo, vocab_size, [multi_text_file])
                save_tokenizer(l2_tokenizer, f"{cwd}/experiments/{vocab_size}/{l1}_{l2}/{l2}_{algo}.json")
                save_tokenizer(l1_l2_tokenizer, f"{cwd}/experiments/{vocab_size}/{l1}_{l2}/{l1}_{l2}_{algo}.json")


            # # Read False Friends data
            # with open(ff_file_path, 'r', encoding='utf-8') as f:
            #     # list of dictionaries
            #     ff_data = list(csv.DictReader(f))
            #
            # analyze_tokenization([l1_tokenizers[algo], l2_tokenizer, l1_l2_tokenizer], ff_data, l1, l2,
            #                      algo, f"{cwd}/analysis/{vocab_size}/{l1}_{l2}/graphs")
            # # write_tokenization_split([l1_tokenizer, l2_tokenizer, l1_l2_tokenizer], ff_data, l1, l2, algo, f"{cwd}/analysis/{l1}_{l2}/tokenization/{l1}_{l2}_{algo}_ff.txt")
            # print(f"finished experiment {l1}_{l2}_{algo}")
    